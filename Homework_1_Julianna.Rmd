---
title: "Homework 1 - Julianna"
author: "Julianna Szabo"
date: "3/16/2021"
output: html_document
---

```{r, include=FALSE}
# Clear memory
rm(list=ls())

# Libraries

library(tidyverse)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(xgboost)
library(randomForest)
library(caret)
theme_set(theme_minimal())

library(h2o)
h2o.init()
h2o.no_progress()
#h2o.shutdown()

my_seed <- (5678)
```

# Exercise 1

```{r cars}
summary(cars)
```




# Exercise 2

First, we are loading the data.

```{r data loading 2}
data <- as_tibble(ISLR::Hitters) %>%
  drop_na(Salary) %>%
  mutate(log_salary = log(Salary), Salary = NULL)
h2o_data <- as.h2o(data)

y <- "log_salary"
X <- setdiff(names(h2o_data), y)
```

## A, Train 2 Random Forest models

```{r}
## 1, Random forest with 2 variables

rf_2_var <- h2o.randomForest(
  X, y,
  training_frame = h2o_data,
  model_id = "rf_2_var",
  mtries = 2,
  seed = my_seed
)

## 2, Random forest with 10 variables

rf_10_var <- h2o.randomForest(
  X, y,
  training_frame = h2o_data,
  model_id = "rf_2_var",
  mtries = 10,
  seed = my_seed
)
```

### Comparison of variable importance

```{r}
h2o.varimp_plot(rf_2_var)
h2o.varimp_plot(rf_10_var)
```

Looks like the first 2 are the same for the two models. With just two variables all variables (except CAtBat) are more important. CRuns and CRBI also show up high on the list on both models. Variables like Years and Hits show up in the first model but not the second.

## B, Explanation of extreme difference in variable importance

In the one with 10 variables CAtBat is shown to be extremely more important than the others with a jump from almost 1 to 0.6. This could be explained that when 10 variables are picked randomly then CAtBat will be included in more of the trees. Therefore, its importance crystallizes out easier.

## C, Two GMB models 

```{r}
## GBM sample_rate = 0.1

gbm_srate_01 <- h2o.gbm(
  x = X, y = y,
  model_id = "gbm_srate_01",
  training_frame = h2o_data,
  sample_rate = 0.1,
  seed = my_seed
)

## GBM sample_rate = 1

gbm_srate_1 <- h2o.gbm(
  x = X, y = y,
  model_id = "gbm_srate_1",
  training_frame = h2o_data,
  sample_rate = 1,
  seed = my_seed
)
```

### Comparison of variable importance

```{r}
h2o.varimp_plot(gbm_srate_01)
h2o.varimp_plot(gbm_srate_1)
```

The one where the sample_rate is 1 has the more extreme values. This is because bootstrapping always completely resampled the data and therefore build every unrelated trees. With a sample_rate equal to 0.1 the generated datasets are closed to each other since only 10% of the data gets replaced. This means the data will be more unform and increase variables importance.

# Exercise 3

We load the new data.

```{r}
# Load the data
data <- read_csv("https://raw.githubusercontent.com/Julianna-szabo/DS2_Homework1/main/KaggleV2-May-2016.csv")

# some data cleaning
data <- select(data, -one_of(c("PatientId", "AppointmentID", "Neighbourhood"))) %>%
  janitor::clean_names()

# for binary prediction, the target variable must be a factor + generate new variables
data <- mutate(
  data,
  no_show = factor(no_show, levels = c("Yes", "No")),
  handcap = ifelse(handcap > 0, 1, 0),
  across(c(gender, scholarship, hipertension, alcoholism, handcap), factor),
  hours_since_scheduled = as.numeric(appointment_day - scheduled_day)
)

# clean up a little bit
data <- filter(data, between(age, 0, 95), hours_since_scheduled >= 0) %>%
  select(-one_of(c("scheduled_day", "appointment_day", "sms_received")))

h2o_data <- as.h2o(data)

y <- "no_show"
X <- setdiff(names(h2o_data), y)
```

## A, Create train / validation / test sets, cutting the data into 5% - 45% - 50% parts

```{r}
splitted_data <- h2o.splitFrame(h2o_data, ratios = c(0.05, 0.45), seed = my_seed)
data_train <- splitted_data[[1]]
data_valid <- splitted_data[[2]]
data_test <- splitted_data[[3]]
```

## B, Train a benchmark model of your choice (such as random forest, gbm or glm) evaluate it on the validation set.

```{r}
glm_model <- h2o.glm(
  X, y,
  training_frame = data_train,
  model_id = "lm",
  lambda = 0,
  nfolds = 5,
  seed = my_seed,
  keep_cross_validation_predictions = TRUE
)

glm_model

h2o.rmse(h2o.performance(glm_model))
h2o.rmse(h2o.performance(glm_model, data_valid))
```

## C, Build at least 3 models of different families using cross validation, keeping cross validated predictions. You might also try deeplearning.

```{r}
## Random forest

rf_model <- h2o.randomForest(
  X, y,
  training_frame = data_train,
  model_id = "rf",
  ntrees = 200,
  max_depth = 10,
  seed = my_seed,
  nfolds = 5,
  keep_cross_validation_predictions = TRUE
)

## GBM

gbm_model <- h2o.gbm(
  X, y,
  training_frame = data_train,
  model_id = "gbm",
  ntrees = 200,
  max_depth = 5,
  learn_rate = 0.1,
  seed = my_seed,
  nfolds = 5,
  keep_cross_validation_predictions = TRUE
)

## Deep learning

deeplearning_model <- h2o.deeplearning(
  X, y,
  training_frame = data_train,
  model_id = "deeplearning",
  hidden = c(32, 8),
  seed = my_seed,
  nfolds = 5,
  keep_cross_validation_predictions = TRUE
)
```

## D, Evaluate validation set performance of each model

```{r}
getPerformanceMetrics <- function(model, newdata = NULL, xval = FALSE) {
  h2o.performance(model, newdata = newdata, xval = xval)@metrics$thresholds_and_metric_scores %>%
    as_tibble() %>%
    mutate(model = model@model_id)
}

plotROC <- function(performance_df) {
  ggplot(performance_df, aes(fpr, tpr, color = model)) +
    geom_path() +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    coord_fixed() +
    labs(x = "False Positive Rate", y = "True Positive Rate")
}

plotRP <- function(performance_df) {
  ggplot(performance_df, aes(precision, tpr, color = model)) +  # tpr = recall
    geom_line() +
    labs(x = "Precision", y = "Recall (TPR)")
}

my_models <- list(glm_model, rf_model, gbm_model, deeplearning_model)
all_performance <- map_df(c(my_models), getPerformanceMetrics, xval = TRUE)
plotROC(all_performance)
plotRP(all_performance)
```

## E, How large are the correlations of predicted scores of the validation set produced by the base learners?




## F, Create a stacked ensemble model from the base learners

```{r}
ensemble_model_glm <- h2o.stackedEnsemble(
  X, y,
  training_frame = data_train,
  metalearner_algorithm = "glm",
  model_id = "stacked_model",
  base_models = my_models
)
```

## G, Evaluate ensembles on validation set. Did it improve prediction?

```{r}
map_df(
  c(my_models, ensemble_model_glm),
  ~{tibble(model = .@model_id, auc = h2o.auc(h2o.performance(., newdata = data_valid)))}
)
```

## H, Evaluate the best performing model on the test set. How does performance compare to that of the validation set?

```{r}
map_df(
  c(ensemble_model_glm),
  ~{tibble(model = .@model_id, auc = h2o.auc(h2o.performance(., newdata = data_test)))}
)
```

The performance of the model on the test set is exactly the same as before. This is a really good sign, since that means that it should be good for generalization on this dataset.

