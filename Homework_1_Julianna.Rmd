---
title: "Homework 1 - Julianna"
author: "Julianna Szabo"
date: "3/16/2021"
output: html_document
---

```{r, include=FALSE}
# Clear memory
rm(list=ls())

# Libraries

library(tidyverse)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(xgboost)
library(randomForest)
library(data.tree)
library(caret)
theme_set(theme_minimal())

library(h2o)
h2o.init()
h2o.no_progress()
#h2o.shutdown()

my_seed <- (5678)
```

# Exercise 1

We have to import the data
```{r cars}
data <- as_tibble(ISLR::OJ)
h2o_data <- as.h2o(data)
```

## A, Train and test data and build a decision tree for reference

```{r}
## Train and test data
splitted_data <- h2o.splitFrame(h2o_data, ratios = 0.75, seed = my_seed)
data_train <- splitted_data[[1]]
data_test <- splitted_data[[2]]
```

```{r}
y <- "Purchase"
X <- setdiff(names(h2o_data), y)

## Simple decision tree
gbm_params_1tree = list(max_depth = seq(2, 10))

### Train and validate a cartesian grid of GBMs
gbm_grid = h2o.grid("gbm", x = X, y = y,
                    grid_id = "gbm_grid_1tree",
                    training_frame = data_train,
                    ntrees = 1, min_rows = 1, 
                    sample_rate = 1, col_sample_rate = 1,
                    learn_rate = .01, seed = my_seed,
                    hyper_params = gbm_params_1tree)

gbm_gridperf = h2o.getGrid(grid_id = "gbm_grid_1tree",
                           sort_by = "auc",
                           decreasing = TRUE)

ggplot(as.data.frame(sapply(gbm_gridperf@summary_table, as.numeric))) +
  geom_point(aes(max_depth, auc)) +
  geom_line(aes(max_depth, auc, group=1)) +
  labs(x="max depth", y="AUC", title="Grid Search for Single Tree Models")

data_1tree = 
  h2o.gbm(x = X, y = y, 
          training_frame = data_train,
          model_id = "data_1tree",
          ntrees = 1, min_rows = 1, 
          sample_rate = 1, col_sample_rate = 1,
          max_depth = 4,
          stopping_rounds = 3, stopping_tolerance = 0.01, 
          stopping_metric = "AUC", 
          seed = 1)

dataH2oTree = h2o.getModelTree(model = data_1tree, tree_number = 1)

source("Plotting_h2o_tree.R")

DataTree = createDataTree(dataH2oTree)

GetEdgeLabel <- function(node) {return (node$edgeLabel)}
GetNodeShape <- function(node) {switch(node$type, 
                                       split = "diamond", leaf = "oval")}
GetFontName <- function(node) {switch(node$type, 
                                      split = 'Palatino-bold', 
                                      leaf = 'Palatino')}
SetEdgeStyle(DataTree, fontname = 'Palatino-italic', 
             label = GetEdgeLabel, labelfloat = TRUE,
             fontsize = "26", fontcolor='royalblue4')
SetNodeStyle(DataTree, fontname = GetFontName, shape = GetNodeShape, 
             fontsize = "26", fontcolor='royalblue4',
             height="0.75", width="1")

SetGraphStyle(DataTree, rankdir = "LR", dpi=70.)

plot(DataTree, output = "graph")

```

## B, Tree ensemble models

```{r}
## Random forest

rf_params <- list(
  ntrees = c(10, 50, 100, 300),
  mtries = c(2, 4, 6, 8),
  sample_rate = c(0.2, 0.632, 1),
  max_depth = c(10, 20)
)

rf_grid <- h2o.grid(
  "randomForest", 
  x = X, y = y,
  training_frame = data_train,
  grid_id = "rf",
  nfolds = 5,
  seed = my_seed,
  hyper_params = rf_params
)

h2o.getGrid(rf_grid@grid_id, "rmse")
best_rf <- h2o.getModel(
  h2o.getGrid(rf_grid@grid_id, "rmse")@model_ids[[1]]
)

best_rf
h2o.rmse(h2o.performance(best_rf))

## GBM

gbm_params <- list(
  learn_rate = c(0.01, 0.05, 0.1, 0.3),
  ntrees = c(10, 50, 100, 300),
  max_depth = c(2, 5),
  sample_rate = c(0.2, 0.5, 0.8, 1)
)

gbm_grid <- h2o.grid(
  "gbm", x = X, y = y,
  grid_id = "gbm",
  training_frame = data_train,
  nfolds = 5,
  seed = my_seed,
  hyper_params = gbm_params
)

h2o.getGrid(gbm_grid@grid_id, "rmse")
best_gbm <- h2o.getModel(
  h2o.getGrid(gbm_grid@grid_id, "rmse")@model_ids[[1]]
)

best_gbm
h2o.rmse(h2o.performance(best_gbm))

## XG Boost

xgboost_params <- list(
  learn_rate = c(0.1, 0.3), 
  ntrees = c(50, 100),
  max_depth = c(2, 5),
  gamma = c(0, 1, 2),
  sample_rate = c(0.5, 1)
)

xgboost_grid <- h2o.grid(
  "xgboost", x = X, y = y,
  grid_id = "xgboost",
  training_frame = data_train,
  nfolds = 5,
  seed = my_seed,
  hyper_params = xgboost_params
)


h2o.getGrid(xgboost_grid@grid_id, "rmse")
best_xgboost <- h2o.getModel(
  h2o.getGrid(xgboost_grid@grid_id, "rmse")@model_ids[[1]]
)

best_xgboost
h2o.rmse(h2o.performance(best_xgboost))
```

## C, Model comparison

```{r}
my_models <- list(
  data_1tree, best_rf, best_gbm, best_xgboost
)
rmse_on_validation <- map_df(my_models, ~{
  tibble(model = .@model_id, RMSE = h2o.rmse(h2o.performance(., data_test)))
}) %>% arrange(RMSE)

rmse_on_validation
```

## D, Plot ROC curve for the best model

```{r}
plotROC <- function(performance_df) {
  ggplot(performance_df, aes(fpr, tpr, color = model)) +
    geom_path() +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    coord_fixed() +
    labs(x = "False Positive Rate", y = "True Positive Rate")
}

plotROC(  )
```

## E, Show variable importance

```{r}
h2o.varimp_plot(  )
```





# Exercise 2

First, we are loading the data.

```{r data loading 2}
data <- as_tibble(ISLR::Hitters) %>%
  drop_na(Salary) %>%
  mutate(log_salary = log(Salary), Salary = NULL)
h2o_data <- as.h2o(data)

y <- "log_salary"
X <- setdiff(names(h2o_data), y)
```

## A, Train 2 Random Forest models

```{r}
## 1, Random forest with 2 variables

rf_2_var <- h2o.randomForest(
  X, y,
  training_frame = h2o_data,
  model_id = "rf_2_var",
  mtries = 2,
  seed = my_seed
)

## 2, Random forest with 10 variables

rf_10_var <- h2o.randomForest(
  X, y,
  training_frame = h2o_data,
  model_id = "rf_2_var",
  mtries = 10,
  seed = my_seed
)
```

### Comparison of variable importance

```{r}
h2o.varimp_plot(rf_2_var)
h2o.varimp_plot(rf_10_var)
```

Looks like the first 2 are the same for the two models. With just two variables all variables (except CAtBat) are more important. CRuns and CRBI also show up high on the list on both models. Variables like Years and Hits show up in the first model but not the second.

## B, Explanation of extreme difference in variable importance

In the one with 10 variables CAtBat is shown to be extremely more important than the others with a jump from almost 1 to 0.6. This could be explained that when 10 variables are picked randomly then CAtBat will be included in more of the trees. Therefore, its importance crystallizes out easier.

## C, Two GMB models 

```{r}
## GBM sample_rate = 0.1

gbm_srate_01 <- h2o.gbm(
  x = X, y = y,
  model_id = "gbm_srate_01",
  training_frame = h2o_data,
  sample_rate = 0.1,
  seed = my_seed
)

## GBM sample_rate = 1

gbm_srate_1 <- h2o.gbm(
  x = X, y = y,
  model_id = "gbm_srate_1",
  training_frame = h2o_data,
  sample_rate = 1,
  seed = my_seed
)
```

### Comparison of variable importance

```{r}
h2o.varimp_plot(gbm_srate_01)
h2o.varimp_plot(gbm_srate_1)
```

The one where the sample_rate is 1 has the more extreme values. This is because bootstrapping always completely resampled the data and therefore build every unrelated trees. With a sample_rate equal to 0.1 the generated datasets are closed to each other since only 10% of the data gets replaced. This means the data will be more unform and increase variables importance.

# Exercise 3

We load the new data.

```{r}
# Load the data
data <- read_csv('https://raw.githubusercontent.com/Julianna-szabo/DS2_Homework1/main/KaggleV2-May-2016.csv')

# some data cleaning
data <- select(data, -one_of(c('PatientId', 'AppointmentID', 'Neighbourhood'))) %>%
  janitor::clean_names()

# for binary prediction, the target variable must be a factor + generate new variables
data <- mutate(
  data,
  no_show = factor(no_show, levels = c('Yes', 'No')),
  handcap = ifelse(handcap > 0, 1, 0),
  across(c(gender, scholarship, hipertension, alcoholism, handcap), factor),
  hours_since_scheduled = as.numeric(appointment_day - scheduled_day)
)

# clean up a little bit
data <- filter(data, between(age, 0, 95), hours_since_scheduled >= 0) %>%
  select(-one_of(c('scheduled_day', 'appointment_day', 'sms_received')))

h2o_data <- as.h2o(data)
```

## A, Create train / validation / test sets, cutting the data into 5% - 45% - 50% parts

```{r}
splitted_data <- h2o.splitFrame(h2o_data, ratios = c(0.05, 0.45), seed = my_seed)
data_train <- splitted_data[[1]]
data_valid <- splitted_data[[2]]
data_test <- splitted_data[[3]]
```

## B, Train a benchmark model of your choice (such as random forest, gbm or glm) evaluate it on the validation set.

```{r}
y <- 'no_show'
X <- setdiff(names(h2o_data), y)

glm_model <- h2o.glm(
  X, y,
  training_frame = data_train,
  model_id = "lm",
  lambda = 0,
  nfolds = 5,
  seed = my_seed,
  keep_cross_validation_predictions = TRUE
)

glm_model

h2o.rmse(h2o.performance(glm_model))
h2o.rmse(h2o.performance(glm_model, data_valid))
```
Looks like this is a good model. The RMSE is 0.44 in the training set which is relatively good, especially because it doesn't change much to the validation set. IT generated more False Positives than False Negatives. This is good because it means that we do not double book appointments. However, we only predict 18% of the ones that do not show up correctly. This is rather poor. Hopefully, it can be improved. 


## C, Build at least 3 models of different families using cross validation, keeping cross validated predictions. You might also try deeplearning.

```{r}
## Random forest

rf_model <- h2o.randomForest(
  X, y,
  training_frame = data_train,
  model_id = 'rf_model',
  ntrees = 200,
  max_depth = 10,
  seed = my_seed,
  nfolds = 5,
  keep_cross_validation_predictions = TRUE
)

## GBM

gbm_model <- h2o.gbm(
  X, y,
  training_frame = data_train,
  model_id = 'gbm_model',
  ntrees = 200,
  max_depth = 5,
  learn_rate = 0.1,
  seed = my_seed,
  nfolds = 5,
  keep_cross_validation_predictions = TRUE
)

## Deeplearning

deeplearning_model <- h2o.xgboost_model(
  X, y,
  training_frame = data_train,
  model_id = 'deeplearning',
  hidden = c(32, 8),
  seed = my_seed,
  nfolds = 5,
  keep_cross_validation_predictions = TRUE
)
```

## D, Evaluate validation set performance of each model

```{r}
getPerformanceMetrics <- function(model, newdata = NULL, xval = FALSE) {
  h2o.performance(model, newdata = newdata, xval = xval)@metrics$thresholds_and_metric_scores %>%
    as_tibble() %>%
    mutate(model = model@model_id)
}

plotROC <- function(performance_df) {
  ggplot(performance_df, aes(fpr, tpr, color = model)) +
    geom_path() +
    geom_abline(slope = 1, intercept = 0, linetype = 'dashed') +
    coord_fixed() +
    labs(x = 'False Positive Rate', y = 'True Positive Rate')
}

plotRP <- function(performance_df) {
  ggplot(performance_df, aes(precision, tpr, color = model)) +  # tpr = recall
    geom_line() +
    labs(x = 'Precision', y = 'Recall (TPR)')
}

my_models <- list(glm_model, rf_model, gbm_model, deeplearning_model)
all_performance <- map_df(c(my_models), getPerformanceMetrics, xval = TRUE)
plotROC(all_performance)
plotRP(all_performance)
```

Looks like the deeplearning mode is performing worse than the others. Random Forest seems to be the best looking at the ROC curve. Looks like the Precision is similar to all the models, but there is a large difference in Recall. In Recall overall Random Forest seems to be the highest while the others only have one spike and then drop.

## E, How large are the correlations of predicted scores of the validation set produced by the base learners?




## F, Create a stacked ensemble model from the base learners

```{r}
ensemble_model_glm <- h2o.stackedEnsemble(
  X, y,
  training_frame = data_train,
  metalearner_algorithm = 'glm',
  model_id = 'stacked_model_glm',
  base_models = my_models
)

ensemble_model_rf <- h2o.stackedEnsemble(
  X, y,
  training_frame = data_train,
  metalearner_algorithm = 'drf',
  model_id = 'stacked_model_rf',
  base_models = my_models
)

ensemble_model_gbm <- h2o.stackedEnsemble(
  X, y,
  training_frame = data_train,
  metalearner_algorithm = 'gbm',
  model_id = 'stacked_model_gbm',
  base_models = my_models
)
```

## G, Evaluate ensembles on validation set. Did it improve prediction?

```{r}
ensemble_models <- list(ensemble_model_glm, ensemble_model_rf, ensemble_model_gbm)
map_df(
  c(my_models, ensemble_models),
  ~{tibble(model = .@model_id, auc = h2o.auc(h2o.performance(., newdata = data_valid)))}
)
```

Looks like the stacked model using glm as a base is the best model since it has the highest AUC, with the deeplearning model close behind. 

## H, Evaluate the best performing model on the test set. How does performance compare to that of the validation set?

```{r}
map_df(
  c(ensemble_model_glm),
  ~{tibble(model = .@model_id, auc = h2o.auc(h2o.performance(., newdata = data_test)))}
)
```

The performance of the model on the test set is exactly the same as before. This is a really good sign, since that means that it should be good for generalization on this dataset.

